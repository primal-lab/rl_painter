Original CLIP model 
https://github.com/openai/CLIP/blob/main/clip/clip.py
preprocess in load but preprocess is basically _transform


preprocess cpu definition - 

def _transform(n_px):
    return Compose([
        Resize(n_px, interpolation=BICUBIC),
        CenterCrop(n_px),
        _convert_image_to_rgb,
        ToTensor(),
        Normalize((0.48145466, 0.4578275, 0.40821073),
                  (0.26862954, 0.26130258, 0.27577711)),
    ])

explanation - 
1. Resize(n_px, interpolation=BICUBIC)
- If given a single integer n_px, torchvision’s Resize scales the image so that the shortest side becomes exactly n_px, preserving aspect ratio.
- Bicubic interpolation = smooth resampling.

2. CenterCrop(n_px)
- Takes a centered square crop of size n_px × n_px.
- After the Resize, the shortest side is already n_px, so this crop simply trims the longer side to make the image square.

3. _convert_image_to_rgb
- Converts to 3‑channel RGB. (PIL .convert("RGB").)

4. ToTensor()
- Converts PIL → PyTorch tensor shaped [C, H, W].
- Scales pixel values from [0, 255] to [0, 1] (float32).

5. Normalize(mean, std)
- Applies channel‑wise normalization with CLIP’s training statistics:
- mean = (0.48145466, 0.4578275, 0.40821073)
- std = (0.26862954, 0.26130258, 0.27577711)

* COMPOSE helps apply each transform in order



CPU → GPU

Resize(n_px, interpolation=BICUBIC)
→ GPU: compute scale = n_px / min(H, W) then
F.interpolate(x, size=(round(H*scale), round(W*scale)), mode="bicubic", align_corners=False)

CenterCrop(n_px)
→ GPU: compute top/left and slice x[:, :, top:top+n_px, left:left+n_px]

_convert_image_to_rgb
→ GPU: if grayscale, x = x.repeat(1, 3, 1, 1)

ToTensor()
→ GPU: ensure tensor input; if max > 1.5, divide by 255 to get [0,1]
(ToTensor also permutes HWC→CHW for PIL; you already have tensors in CHW, so no permute needed.)

Normalize(mean, std)
→ GPU: (x - mean) / std
